# Speech-Emotion-Recognition

* Speech Emotion Recognition (SER) is the process of identifying emotions from speech signals.
* SER is based on the fact that voice often reflects underlying emotion through tone and pitch.
* SER is a rapidly growing research domain in recent years, with applications in human-computer interaction and reducing the need for human intervention.
* SER can be achieved using machine learning techniques like Multilayer Perceptron Classifier (MLP Classifier), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN) 
  with Long Short-Term Memory (LSTM) layers.
* These techniques are used to extract features from speech signals, such as Mel-frequency cepstrum coefficients (MFCC), chroma, and mel features, and train the model to recognize emotions.
* The RAVDESS dataset is commonly used in SER projects, which contains around 1500 audio file inputs from 24 different actors who vocalize two lexically-matched statements in a neutral 
  North American accent.
* The dataset contains seven emotions, including neutral, calm, happy, sad, angry, fearful, disgust, and surprised, with normal and strong emotional intensity.
* K-Fold Cross Validation and other evaluation techniques are used to assess the performance of the SER model.
* The model is trained on the training data and tested on test data, followed by testing on live audio input (unseen) to collect results.
* LSTM-based RNNs are effective in modeling long-range sequential text data, making them suitable for speech-based classification tasks.
* CNNs are effective in capturing spatial patterns in data, making them suitable for speech emotion recognition.

